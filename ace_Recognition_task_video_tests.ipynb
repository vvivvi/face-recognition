{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "znzWDP4N8cRn"
   },
   "source": [
    "# Face recognition using neural network features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MUey7NiG8cRq"
   },
   "source": [
    "In this task, you have to construct face recognizer based on features extracted from the neural network. The task consists of two parts: image classification and video classification. In the first one you should classify distinct images and in the second one you will deal with short video sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 275
    },
    "colab_type": "code",
    "id": "q-DupBs-wz6F",
    "outputId": "87169e0c-17ad-4e4e-a1b8-3dad6d5926fd"
   },
   "outputs": [],
   "source": [
    "#!pip install mtcnn\n",
    "#!conda install scikit-learn\n",
    "#!conda install scikit-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sJgKREpP8cRs"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, Activation\n",
    "from tensorflow.keras.layers import Convolution2D, MaxPooling2D\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "lZrAaqQw8cRx",
    "outputId": "21fdcebd-26cb-4a79-dcb7-2c589a4aeda2",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\IPython\\core\\magics\\pylab.py:160: UserWarning: pylab import has clobbered these variables: ['imread']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from copy import copy\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7SZ19PNFAaI_"
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "def unpack(filename):\n",
    "    with zipfile.ZipFile(filename) as zf:\n",
    "        zf.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 527
    },
    "colab_type": "code",
    "id": "F_C8UHF2Am2Z",
    "outputId": "2521d485-c62c-43ae-85c8-b531103deeb9"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "d = !pwd\n",
    "assignment_dir = d[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "njBxmkGt8cR0"
   },
   "source": [
    "First of all, you have you have to read the data. Run the cell below to unpack data."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/vvi/repos/coursera/HSE/face-recognition'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from get_data import unpack\n",
    "#unpack(assignment_dir + '/Face_Recognition_data.zip')\n",
    "assignment_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4BUXXCYBKlwy"
   },
   "outputs": [],
   "source": [
    "images_processed=True\n",
    "videos_processed=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pRnR7EDWvO3Y"
   },
   "outputs": [],
   "source": [
    "# !unzip '/content/drive/My Drive/face-recognition/Face_Recognition_data.zip' -d '/content/drive/My Drive/face-recognition'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q400JXHp8cR4"
   },
   "source": [
    "### Reading data for image and video classification (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2kVl390l8cR5"
   },
   "source": [
    "Implement function $\\tt{load}$\\_$\\tt{image}$\\_$\\tt{data}$. It should return a tuple of 4 dictionaries: the first two are training images and labels, the second two are testing ones. The keys of the dictionaries are the names of the image files and the values are 3-dimensional numpy arrays (for images) or strings with the names (for labels).\n",
    "\n",
    "$\\tt{dir}$\\_$\\tt{name}$ is the name of directory with data for image classification. If 'Face_Recofnition_data' directory is located in the same directory as this notebook, then the default value can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U_C6qbsj8cR6"
   },
   "outputs": [],
   "source": [
    "from skimage import io\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def load_image_data(dir_name):\n",
    "    \"\"\"Your implementation\"\"\"\n",
    "    images_train={}\n",
    "    images_test={}\n",
    "    labels_train={}\n",
    "    labels_test={}\n",
    "\n",
    "    for im_path in sorted(glob.glob(dir_name + \"/train/images/*.jpg\")):\n",
    "      images_train[im_path.split('/')[-1]] = io.imread(im_path)\n",
    "    for im_path in sorted(glob.glob(dir_name + \"/test/images/*.jpg\")):\n",
    "      images_test[im_path.split('/')[-1]] = io.imread(im_path)  \n",
    "\n",
    "    labels=pd.read_csv(dir_name+'/train/y_train.csv')\n",
    "    for rowidx in range(labels.shape[0]):\n",
    "      labels_train[labels.iloc[rowidx,0]] = labels.iloc[rowidx,1]\n",
    "\n",
    "\n",
    "    labels=pd.read_csv(dir_name+'/test/y_test.csv')\n",
    "    for rowidx in range(labels.shape[0]):\n",
    "      labels_test[labels.iloc[rowidx,0]] = labels.iloc[rowidx,1]\n",
    "    \n",
    "    \n",
    " \n",
    "    return images_train, labels_train, images_test, labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TloxdDpK8cR9",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "661 \ttraining images\n",
      "808 \ttesting images\n"
     ]
    }
   ],
   "source": [
    "if not images_processed:\n",
    "  x_train, y_train, x_test, y_test = load_image_data(assignment_dir + \"/Face_Recognition_data/image_classification\")\n",
    "  np.savez_compressed(assignment_dir+'/loaded_images.npz',x_train=x_train,y_train=y_train,x_test=x_test,y_test=y_test)\n",
    "else:\n",
    "  with np.load(assignment_dir + \"/loaded_images.npz\", allow_pickle=True) as data:\n",
    "    x_train = data['x_train'].item()\n",
    "    y_train = data['y_train'].item()\n",
    "    x_test = data['x_test'].item()\n",
    "    y_test = data['y_test'].item()\n",
    "    \n",
    "print(len(x_train), '\\ttraining images')\n",
    "print(len(x_test), '\\ttesting images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nSFbVqqq8cSB"
   },
   "outputs": [],
   "source": [
    "def visualize(data, labels, function = lambda x:x[0], n_cols = 5, n_rows=1):\n",
    "    figure(figsize = (3*n_cols,3*n_rows))\n",
    "    for n,i in enumerate(np.random.choice(list(data.keys()), size = n_cols*n_rows)):\n",
    "        plt.subplot(n_rows,n_cols,n+1)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(function([data[i]]))\n",
    "        plt.title(labels[i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eKL8sKaV8cSE"
   },
   "source": [
    "That is how the data looks like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dJJFsX1x8cSI"
   },
   "source": [
    "Let us now read the video classification data, as well. You have to implement function to load video data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0fTf4Edj8cSJ"
   },
   "source": [
    "Function $\\tt{load}$\\_$\\tt{video}$\\_$\\tt{data}$ should also return a tuple of 4 dictionaries: the first two are training images and labels, the second two are testing videos and labels. The training data is in the same format as in image classification task. The keys of testing data and labels are video ids and the values are the lists of frames and the strings with names, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TpxUKpip8cSK"
   },
   "outputs": [],
   "source": [
    "def load_video_data(dir_name = 'Face_Recognition_data/video_classification'):\n",
    "    images_train={}\n",
    "    videos_test={}\n",
    "    labels_train={}\n",
    "    labels_test={}\n",
    "\n",
    "    for im_path in sorted(glob.glob(dir_name + \"/train/images/*.jpg\")):\n",
    "      images_train[im_path.split('/')[-1]] = io.imread(im_path)\n",
    "    \n",
    "    labels=pd.read_csv(dir_name+'/train/y_train.csv')\n",
    "    for rowidx in range(labels.shape[0]):\n",
    "      labels_train[str(labels.iloc[rowidx,0])] = labels.iloc[rowidx,1]\n",
    "\n",
    "    for im_path in sorted(glob.glob(dir_name + \"/test/videos/*/\")):\n",
    "      video_id=im_path.split('/')[-2]\n",
    "      print(video_id)\n",
    "      filenames = glob.glob(dir_name + \"/test/videos/\"+video_id+\"/*.jpg\")\n",
    "      sorted_filenames = sorted(filenames, key=lambda x:int(x.split('/')[-1][:-4]))\n",
    "      videos_test[video_id] = []\n",
    "      for fn in sorted_filenames:\n",
    "        videos_test[video_id].append(io.imread(fn))\n",
    "      \n",
    "\n",
    "    labels=pd.read_csv(dir_name+'/test/y_test.csv')\n",
    "    for rowidx in range(labels.shape[0]):\n",
    "      labels_test[str(labels.iloc[rowidx,0])] = labels.iloc[rowidx,1]\n",
    "\n",
    "      # images_test[im_path.split('/')[-1]] = io.imread(im_path)  \n",
    "  \n",
    "\n",
    "    return images_train, labels_train, videos_test, labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vVGLJte58cSN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "729 \ttraining images\n",
      "70 \ttesting videos\n"
     ]
    }
   ],
   "source": [
    "if not videos_processed:\n",
    "  video_train, train_labels, video_test, test_labels = load_video_data(assignment_dir + \"/Face_Recognition_data/video_classification\")\n",
    "  np.savez_compressed(assignment_dir+'/loaded_videos.npz',video_train=video_train, train_labels=train_labels, \n",
    "                    video_test=video_test, test_labels=test_labels)\n",
    "else:\n",
    "  with np.load(assignment_dir + \"/loaded_videos.npz\", allow_pickle=True) as data:\n",
    "    video_train = data['video_train'].item()\n",
    "    train_labels = data['train_labels'].item()\n",
    "    video_test = data['video_test'].item()\n",
    "    test_labels = data['test_labels'].item()  \n",
    "\n",
    "\n",
    "print(len(video_train), '\\ttraining images')\n",
    "print(len(video_test), '\\ttesting videos')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8xy3POFf8cSW"
   },
   "source": [
    "### Preprocessing (3 points)\n",
    "You have to implement preprocessing function in the cell below.\n",
    "Getting a list of images as an input the this function should detect the face on each image, find the facial keypoints () and then crop and normalize the image according to these keypoints. The output of this function is the list of images which contain only the aligned face and should be converted to the tensor of the shape $(N, 224, 224, 3)\\ $ where $N$ is the length of the initial list. You can add extra arguments to the preprocess function if necessary (i.e. flag $\\tt{is}$\\_$\\tt{video}$ to determine if the list of images is video sequence or not).\n",
    "\n",
    "For face detection and facial keypoint regression you can use your models from the previous tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DxnViArxRpNB"
   },
   "source": [
    "\n",
    "---\n",
    "From the above wording of the instructions, we conclude that we are not forced to use the models from previous tasks but can use also other models of our choosing for the pre-processing task.\n",
    "\n",
    "Exactly this we will do as we anticipate a heap of problems with the previous models, as those models have been trained for fixed resolution and with just small datasets. Instead, we base the pre-processing on face and facial landmark detections of the MTCNN model of the ipazc/MTCNN project (http://github.com/ipazc/mtcnn).\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
   },
   "outputs": [],
   "source": [
    "# from week 1 assignment\n",
    "from skimage import transform\n",
    "\n",
    "def transform_face(image, eyes, size_multiplier = 2.15):\n",
    "    e=np.array(eyes)\n",
    "    center = np.mean(e,axis=0, keepdims=True)\n",
    "    diff = e[1]-e[0]\n",
    "    dist=np.sqrt(np.sum(diff*diff))\n",
    "    angle = np.arctan2(diff[1],diff[0])\n",
    "    \n",
    "    img_rotated = transform.rotate(image,np.degrees(angle), center=center)\n",
    "\n",
    "    #plt.imshow(img_rotated)\n",
    "    #print('Rotated image')\n",
    "    #plt.show()\n",
    "\n",
    "    face_extent = int(size_multiplier*dist) \n",
    "\n",
    "    # print('Face extent: ', face_extent)\n",
    "\n",
    "    box_left = int(center[0,0]-face_extent) \n",
    "    box_right = int(box_left + 2* face_extent)\n",
    "\n",
    "    box_top = int(center[0,1]-face_extent) \n",
    "    box_bottom = int(box_top + 2* face_extent)\n",
    "\n",
    "    pad=max(0,-box_left,-box_top,box_bottom-image.shape[0]-1, box_right-image.shape[1]-1)\n",
    "    img_crop = np.pad(img_rotated, ((pad,pad),(pad,pad),(0,0)))[box_top+pad:box_bottom+1+pad, box_left+pad:box_right+1+pad]\n",
    "\n",
    "    BOTTLENECK_SIZE = 90\n",
    "    img_crop = transform.resize(img_crop, (BOTTLENECK_SIZE, BOTTLENECK_SIZE))\n",
    "    \n",
    "    return transform.resize(img_crop, (224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4gcqFMJK8cSX"
   },
   "outputs": [],
   "source": [
    "import scipy.misc\n",
    "from mtcnn.mtcnn import MTCNN\n",
    "from matplotlib.patches import Rectangle, Circle\n",
    "\n",
    "from skimage import exposure\n",
    "from skimage import filters\n",
    "\n",
    "import copy\n",
    "\n",
    "def equalize_color_img(img):\n",
    "  img_eq = np.zeros_like(img)\n",
    "  img_eq[:,:,0] = exposure.equalize_hist(img[:,:,0])\n",
    "  img_eq[:,:,1] = exposure.equalize_hist(img[:,:,1])\n",
    "  img_eq[:,:,2] = exposure.equalize_hist(img[:,:,2])\n",
    "  return img_eq\n",
    "\n",
    "def distance_from_center(img, detection):\n",
    "    # img is numpay array\n",
    "    # detection is the face detection object from MTCNN detector\n",
    "\n",
    "    x, y, width, height = detection['box']\n",
    "    center_x = (img.shape[1]-1)/2.0\n",
    "    center_y = (img.shape[0]-1)/2.0\n",
    "    face_x = x + width/2.0\n",
    "    face_y = y + height/2.0\n",
    "    dx = center_x - face_x\n",
    "    dy = center_y - face_y\n",
    "    return np.sqrt(dx*dx+dy*dy)\n",
    "\n",
    "def measure_blur(img):\n",
    "  return np.var(filters.laplace(img[:,:,0])) + np.var(filters.laplace(img[:,:,1])) + np.var(filters.laplace(img[:,:,2]))\n",
    "\n",
    "def preprocess_imgs(imgs, size_multiplier=2.15):\n",
    "    ret_imgs=[]\n",
    "    detector = MTCNN()\n",
    "    n_images = len(imgs)\n",
    "    processed_img = 0\n",
    "\n",
    "    for img in imgs:\n",
    "      processed_img += 1\n",
    "      if processed_img%10 == 1: \n",
    "          print ('Preprocessing image {}/{}'.format(processed_img,n_images))\n",
    "      #plt.imshow(img)\n",
    "      #plt.title('Source image')\n",
    "      #plt.show()\n",
    "\n",
    "      faces = detector.detect_faces(img)\n",
    "      #for face in faces:\n",
    "\t    #  print(face)\n",
    "       \n",
    "      #choose the face that is closest to the image center\n",
    "      if len(faces) < 1:\n",
    "        ret_imgs.append(np.zeros((224,224,3)))\n",
    "        continue\n",
    "\n",
    "      min_ind=0\n",
    "      min_dist = distance_from_center(img, faces[0])\n",
    "      for i in range(1,len(faces)):\n",
    "        dist = distance_from_center(img, faces[i])\n",
    "        if dist < min_dist:\n",
    "          min_ind = i\n",
    "          min_dist = dist     \n",
    "       \n",
    "      #plt.imshow(img)\n",
    "      #ax = plt.gca()\n",
    "      face=faces[min_ind]\n",
    "\n",
    "      #x, y, width, height = face['box']\n",
    "      #rect = Rectangle((x, y), width, height, fill=False, color='red')\n",
    "      #ax.add_patch(rect)\n",
    "      #dot = Circle(face['keypoints']['left_eye'], radius=2, color='green')\n",
    "      #ax.add_patch(dot)\n",
    "      #dot = Circle(face['keypoints']['right_eye'], radius=2, color='blue')\n",
    "      #ax.add_patch(dot)\n",
    "      #plt.show() \n",
    "\n",
    "      if face['confidence']< 0.9:\n",
    "        ret_imgs.append(np.zeros((224,224,3)))\n",
    "        continue\n",
    "\n",
    "      eyes=[face['keypoints']['left_eye'], face['keypoints']['right_eye']]  \n",
    "      #print(eyes)\n",
    "      img_transformed = equalize_color_img(transform_face(img, eyes, \n",
    "                                                          size_multiplier=size_multiplier))\n",
    "      \n",
    "      #blur = np.var(filters.laplace(img_transformed[:,:,0])) + np.var(filters.laplace(img_transformed[:,:,1])) + np.var(filters.laplace(img_transformed[:,:,2]))\n",
    "      #print('Blur: ',blur)\n",
    "\n",
    "      img_blurred=copy.deepcopy(img_transformed) \n",
    "\n",
    "      #sigma=1\n",
    "      #for i in range(5):\n",
    "      #  img_blurred=filters.gaussian(img_blurred)\n",
    "      #sharpness = measure_blur(img_blurred)\n",
    "      #MAX_SHARPNESS = 0.0005\n",
    "      #while sharpness > MAX_SHARPNESS:\n",
    "      #  img_blurred=filters.gaussian(img_blurred)\n",
    "      #  sharpness = measure_blur(img_blurred)\n",
    "        #print('Blur after {} iterations of Gaussian blurring: {}'.format(i+1,blur))\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "      #plt.imshow(img_transformed)\n",
    "      #plt.show()\n",
    "\n",
    "      ret_imgs.append(img_blurred)\n",
    "    return ret_imgs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HLIbD5We8cSb"
   },
   "source": [
    "#### Visualization of preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bG1EI8788cSh"
   },
   "source": [
    "The neural network is already trained on the other face dataset. You should use this network as feature extractor to get descriptors of the faces. You can choose any hidden layer you need (or several layers) to extract features and any classification method."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "from tensorflow.keras.models import load_model\n",
    "model = load_model(assignment_dir + '/face_recognition_model.h5')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n9CBRzUJ8cSl"
   },
   "source": [
    "Here is an example of using the network as feature extractor. The shape of input tensor has to be (n_images, 224, 224, 3), so you can input several images simultaneously and get their face descriptors of shape (n_images, n_components)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M-ApWK778cSl"
   },
   "outputs": [],
   "source": [
    "def get_layer_output(images, layer = 'fc7'):\n",
    "    assert len(images.shape)==4, 'Wrong input dimentionality!'\n",
    "    assert images.shape[1:]==(224,224,3), 'Wrong input shape!'\n",
    "    \n",
    "    layers=layer.split('+')\n",
    "    n_img = images.shape[0]\n",
    "\n",
    "    output=np.zeros((images.shape[0],0))\n",
    "\n",
    "    batch_size = 2\n",
    "    \n",
    "    for l in layers: \n",
    "      network_output = model.get_layer(l).output\n",
    "      feature_extraction_model = Model(model.input, network_output)\n",
    "         \n",
    "      layer_output=zeros((n_img,feature_extraction_model.predict(images[0,:,:,:].reshape(1,224,224,3)).shape[1]))\n",
    "    \n",
    "      \n",
    "    \n",
    "      for i in range(int(n_img/batch_size)):\n",
    "        layer_output[i*batch_size:min(n_img,(i+1)*batch_size),:]=feature_extraction_model.predict(\n",
    "            images[i*batch_size:min(n_img,(i+1)*batch_size),:,:,:])    \n",
    "    \n",
    "      output=np.hstack((output,layer_output))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cOaAKwX18cSo"
   },
   "outputs": [],
   "source": [
    "img = cv2.resize(x_train['0.jpg'], (224,224)).reshape(1,224,224,3)\n",
    "out = get_layer_output(img)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vJLDs5QP8cSr"
   },
   "source": [
    "### Training classifier (2 points)\n",
    "\n",
    "\n",
    "You have to implement class $\\tt{Classifier}$ with methods $\\tt{fit}$, $\\tt{classify}$\\_$\\tt{images}$ and $\\tt{classify}$\\_$\\tt{videos}$ in the cell below. \n",
    "The method $\\tt{Classifier.fit}$ gets two dictionaries as input: train images and labels, and trains the classifier to predict the person shown on the image.\n",
    "$\\tt{Classifier.classify}$\\_$\\tt{images}$ gets the dictionary of test images (with filenames as keys) as input and should return the dictionary of the predicted labels.\n",
    "$\\tt{Classifier.classify}$\\_$\\tt{videos}$ is similar to the previous one, but gets the dictionary of test videos (with video as keys) as input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D4-aKpmg8cSt"
   },
   "source": [
    "To classify video you can combine the predictions for its frames any way you want (averaging, voting, etc.).\n",
    "If video classification takes too long you can use face detector not in all the frames but every few frames while preprocessing video frames. \n",
    "Besides, sometimes the face is hardly detected on the image and the frame in which the detector works wrong can add noise to the prediction. Hence, the result of the prediction without using such frames may be better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "taBviW0o8cSx"
   },
   "source": [
    "Now we can build the classifier, fit it and use to predict the labels of testing images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5C4U2NkS8cS0"
   },
   "source": [
    "### Image classification quality (2 points)\n",
    "\n",
    "Let us check the accuracy of your classification. To obtain 1 point for that rubric your implementation must have accuracy at least 0.90, to obtain 2 points — at least 0.95."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "geFa3GNe8cS1"
   },
   "outputs": [],
   "source": [
    "def check_test(output, gt):\n",
    "    correct = 0.\n",
    "    total = len(gt)\n",
    "    for k, v in gt.items():\n",
    "        if output[k] == v:\n",
    "            correct += 1\n",
    "    accuracy = correct / total\n",
    "\n",
    "    return 'Classification accuracy is %.4f' % accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MY1WMDVP8cS6"
   },
   "source": [
    "### Video classification quality (2 points)\n",
    "\n",
    "Let us check the quality of video classification. To obtain 1 point for that rubric your implementation must have accuracy at least 0.80, to obtain 2 points — at least 0.85."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn import decomposition\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "\n",
    "from skimage.io import imread\n",
    "import cv2\n",
    "from os.path import join\n",
    "class ClassifierVideoCatboost():\n",
    "    def __init__(self, nn_model):\n",
    "        self.label_encoder = preprocessing.LabelEncoder()\n",
    "        self.catboost_model = CatBoostClassifier(verbose=True, iterations=3000)\n",
    "        self.pca1 = decomposition.PCA(0.9)\n",
    "        self.pca2 = decomposition.PCA(0.9)\n",
    "        self.pca3 = decomposition.PCA(0.9)\n",
    "        \n",
    "        self.layer = 'fc6+fc7+fc8'\n",
    "        self.size_multiplier=2.15\n",
    "        self.video_step = 1\n",
    "\n",
    "    def fit(self, train_imgs, train_labels):\n",
    "        n_images=len(train_imgs)\n",
    "        image_counter=0\n",
    "        \n",
    "        labels_orig = []\n",
    "        for id in train_imgs:\n",
    "          labels_orig.append(train_labels[id])\n",
    "        \n",
    "        labels_orig.extend(labels_orig)\n",
    "\n",
    "        preprocessed_images = np.array(preprocess_imgs(train_imgs.values(), size_multiplier=self.size_multiplier))\n",
    "        flipped_images=np.flip(preprocessed_images, -2)\n",
    "        augmented_images = np.vstack((preprocessed_images, flipped_images))\n",
    "\n",
    "        # augment images with two different scaling bottlenecks\n",
    "\n",
    "#        bottleneck_70 = np.zeros_like(augmented_images)\n",
    "#        bottleneck_110 = np.zeros_like(augmented_images)\n",
    "##\n",
    "#        n_augm = augmented_images.shape[0]\n",
    "#        for i in range(n_augm):\n",
    "#        bottleneck_70[i,:,:] = transform.resize(transform.resize(augmented_images[i], (70, 70)), (224, 224))\n",
    "#        bottleneck_110[i,:,:] = transform.resize(transform.resize(augmented_images[i], (110, 110)), (224, 224))\n",
    "\n",
    "#        labels_orig = labels_orig * 3\n",
    "#        augmented_images = np.vstack((augmented_images, bottleneck_70, bottleneck_110))\n",
    "        \n",
    "#        blur1 = np.zeros_like(augmented_images)\n",
    "#        blur2 = np.zeros_like(augmented_images)\n",
    "\n",
    "#        n_augm = augmented_images.shape[0]\n",
    "#        for i in range(n_augm):\n",
    "#         blur1[i,:,:] = filters.gaussian(augmented_images[i])\n",
    "#         blur2[i,:,:] = filters.gaussian(blur1[i])\n",
    "         \n",
    "#        labels_orig = labels_orig * 3\n",
    "#        augmented_images = np.vstack((augmented_images, blur1, blur2))\n",
    "\n",
    "#        print('augmented image set shape {}'.format(augmented_images.shape))\n",
    "\n",
    "#        #flip the images of the second half horizontally\n",
    "\n",
    "        pca1 = self.pca1.fit_transform(get_layer_output(augmented_images, layer='fc6'))\n",
    "        pca2 = self.pca2.fit_transform(get_layer_output(augmented_images, layer='fc7'))\n",
    "        pca3 = self.pca3.fit_transform(get_layer_output(augmented_images, layer='fc8'))\n",
    "        # pca2 = self.pca2.fit_transform(get_layer_output(augmented_images,layer='fc7'))\n",
    "        \n",
    "        self.x_train_internal= np.hstack((pca1,pca2,pca3))\n",
    "        self.y_train_internal = self.label_encoder.fit_transform(labels_orig)\n",
    "        \n",
    "        self.catboost_model.fit(self.x_train_internal, self.y_train_internal)\n",
    "        self.fitted=True\n",
    "\n",
    "    def classify_images(self, test_imgs):\n",
    "      assert self.fitted\n",
    "\n",
    "      n_images=len(test_imgs)\n",
    "      image_counter=0\n",
    "              \n",
    "      # x_test= get_layer_output(np.array(preprocess_imgs(test_imgs.values(), size_multiplier=self.size_multiplier)), layer=self.layer)\n",
    "      preprocess_result = np.array(preprocess_imgs(test_imgs.values(), size_multiplier=self.size_multiplier))   \n",
    "      pca1 =  self.pca1.transform(get_layer_output(preprocess_result, layer='fc7'))\n",
    "      pca2 =  self.pca2.transform(get_layer_output(preprocess_result, layer='fc7'))\n",
    "      pca3 =  self.pca3.transform(get_layer_output(preprocess_result, layer='fc8'))\n",
    "      x_test = np.hstack((pca1, pca2, pca3))  \n",
    "      ids=[] \n",
    "      for id in test_imgs:\n",
    "        ids.append(id)\n",
    "        \n",
    "      y_test=self.label_encoder.inverse_transform(self.catboost_model.predict(x_test))\n",
    "      print('mapping predictions back')\n",
    "      print(y_test.shape)\n",
    "      for i in range(n_images):\n",
    "        predictions[ids[i]] = str(y_test[i])\n",
    "      return predictions\n",
    "        \n",
    "\n",
    "\n",
    "      \n",
    "\n",
    "        \n",
    "    def classify_videos(self, test_video):\n",
    "        assert self.fitted\n",
    "        predictions={}\n",
    "\n",
    "        video_step = 6\n",
    "\n",
    "        preprocess_pool=[]\n",
    "        pool_start={}\n",
    "        pool_end={}\n",
    "\n",
    "        # populate pool and preprocess all images in one lump\n",
    "        for id in test_video:\n",
    "          pool_start[id] = len(preprocess_pool) \n",
    "          preprocess_pool.extend(test_video[id][::video_step])\n",
    "          pool_end[id] = len(preprocess_pool) \n",
    "\n",
    "        preprocess_result = np.array(preprocess_imgs(preprocess_pool, size_multiplier=self.size_multiplier))\n",
    "        \n",
    "         \n",
    "        for id in test_video:\n",
    "          print('classifying video id ',id )\n",
    "          pca1 = self.pca1.transform(get_layer_output(preprocess_result[pool_start[id]:pool_end[id]], layer='fc7'))\n",
    "          pca2 = self.pca2.transform(get_layer_output(preprocess_result[pool_start[id]:pool_end[id]], layer='fc7'))\n",
    "          pca3 = self.pca3.transform(get_layer_output(preprocess_result[pool_start[id]:pool_end[id]], layer='fc8'))\n",
    "                                    \n",
    "          x_pred=  np.hstack((pca1, pca2, pca3))\n",
    "          probs = self.catboost_model.predict_proba(x_pred)\n",
    "          y_pred= self.label_encoder.inverse_transform(self.catboost_model.predict(x_pred))\n",
    "          for row in range(probs.shape[0]):\n",
    "         #   print('Frame ',row)\n",
    "             best_indices = probs[row,:].argsort()[-5:][::-1]\n",
    "         #   #print(neigh_dist.shape, neigh_ind.shape)\n",
    "         #   #print('best indices:', best_indices)\n",
    "             for i in range(5):\n",
    "              print('{} (p={})'.format(self.label_encoder.inverse_transform(best_indices)[i],\n",
    "                                       probs[row,best_indices[i]]))\n",
    "\n",
    "#            # collect sliding window of past nearest neighbours\n",
    "#\n",
    "#            windowstart=max(0,row-10)\n",
    "#            ind_window=np.array(neigh_ind[windowstart:row+1]).flatten() \n",
    "#            dist_window=np.array(neigh_dist[windowstart:row+1]).flatten() \n",
    "#\n",
    "#            ind_window_sorted = ind_window[np.argsort(dist_window)]\n",
    "#            dist_window_sorted = np.sort(dist_window)\n",
    "#\n",
    "#            \n",
    "#            print(neigh_dist[row])\n",
    "#            print(self.label_encoder.inverse_transform(self.y_train_internal[neigh_ind[row]]))\n",
    "#            print(self.label_encoder.inverse_transform(self.y_train_internal[ind_window_sorted]))\n",
    "#            print(dist_window_sorted)\n",
    "\n",
    "             print('Frame prediction: ',y_pred[row])\n",
    "\n",
    "          lst=list(y_pred)\n",
    "          predictions[id]=max(set(lst), key=lst.count) # mode of y_pred\n",
    "          print('prediction for video {}: {}'.format(id,predictions[id]))\n",
    "\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TSLiFmtWLgan"
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier as kNN\n",
    "from sklearn import preprocessing\n",
    "from sklearn import decomposition\n",
    "from statistics import mode\n",
    "\n",
    "from skimage.io import imread\n",
    "import cv2\n",
    "from os.path import join\n",
    "class ClassifierVideo():\n",
    "    def __init__(self, nn_model):\n",
    "        self.label_encoder = preprocessing.LabelEncoder()\n",
    "        self.standard_scaler = preprocessing.StandardScaler()\n",
    "        self.fitted = False\n",
    "        self.feature_dim = 4096\n",
    "        self.knn_model = kNN(n_neighbors=5)\n",
    "        self.pca = decomposition.PCA(0.9, whiten=True)\n",
    "        self.pca2 = decomposition.PCA(0.9, whiten=True)\n",
    "        self.layer = 'fc7'\n",
    "        self.size_multiplier=2.15\n",
    "        self.video_step = 1\n",
    "\n",
    "    def fit(self, train_imgs, train_labels):\n",
    "        n_images=len(train_imgs)\n",
    "        image_counter=0\n",
    "        \n",
    "        labels_orig = []\n",
    "        for id in train_imgs:\n",
    "          labels_orig.append(train_labels[id])\n",
    "        \n",
    "        labels_orig.extend(labels_orig)\n",
    "\n",
    "        preprocessed_images = np.array(preprocess_imgs(train_imgs.values(), size_multiplier=self.size_multiplier))\n",
    "        flipped_images=np.flip(preprocessed_images, -2)\n",
    "        augmented_images = np.vstack((preprocessed_images, flipped_images))\n",
    "\n",
    "        # augment images with two different scaling bottlenecks\n",
    "\n",
    "        #bottleneck_70 = np.zeros_like(augmented_images)\n",
    "        #bottleneck_110 = np.zeros_like(augmented_images)\n",
    "#\n",
    "#       n_augm = augmented_images.shape[0]\n",
    "#        for i in range(n_augm):\n",
    "#         bottleneck_70[i,:,:] = transform.resize(transform.resize(augmented_images[i], (70, 70)), (224, 224))\n",
    "#         bottleneck_110[i,:,:] = transform.resize(transform.resize(augmented_images[i], (110, 110)), (224, 224))\n",
    "#\n",
    "#        labels_orig = labels_orig * 3\n",
    "#        augmented_images = np.vstack((augmented_images, bottleneck_70, bottleneck_110))\n",
    "        \n",
    " #       blur1 = np.zeros_like(augmented_images)\n",
    " #       blur2 = np.zeros_like(augmented_images)\n",
    "\n",
    " #       n_augm = augmented_images.shape[0]\n",
    " #       for i in range(n_augm):\n",
    " #        blur1[i,:,:] = filters.gaussian(augmented_images[i])\n",
    " #        blur2[i,:,:] = filters.gaussian(blur1[i])\n",
    "         \n",
    " #       labels_orig = labels_orig * 3\n",
    " #       augmented_images = np.vstack((augmented_images, blur1, blur2))\n",
    "\n",
    "        print('augmented image set shape {}'.format(augmented_images.shape))\n",
    "\n",
    "        #flip the images of the second half horizontally\n",
    "\n",
    "        pca1 = self.pca.fit_transform(get_layer_output(augmented_images,layer=self.layer))\n",
    "        # pca2 = self.pca2.fit_transform(get_layer_output(augmented_images,layer='fc7'))\n",
    "        \n",
    "        self.x_train_internal= pca1\n",
    "        self.y_train_internal = self.label_encoder.fit_transform(labels_orig)\n",
    "        \n",
    "        self.knn_model.fit(self.x_train_internal, self.y_train_internal)\n",
    "        self.fitted=True\n",
    "\n",
    "    def classify_images(self, test_imgs):\n",
    "      assert self.fitted\n",
    "      predictions={}\n",
    "\n",
    "      n_images=len(test_imgs)\n",
    "      image_counter=0\n",
    "              \n",
    "      x_test= self.pca.transform(get_layer_output(np.array(preprocess_imgs(test_imgs.values(), size_multiplier=self.size_multiplier)), layer=self.layer))\n",
    "      ids=[] \n",
    "      for id in test_imgs:\n",
    "        ids.append(id)\n",
    "\n",
    "      y_test=self.label_encoder.inverse_transform(self.knn_model.predict(x_test))\n",
    "      print('mapping predictions back')\n",
    "      print(y_test.shape)\n",
    "      for i in range(n_images):\n",
    "        predictions[ids[i]] = str(y_test[i])\n",
    "      return predictions\n",
    "        \n",
    "\n",
    "\n",
    "      \n",
    "\n",
    "        \n",
    "    def classify_videos(self, test_video):\n",
    "        assert self.fitted\n",
    "        predictions={}\n",
    "\n",
    "        video_step = 6\n",
    "\n",
    "        preprocess_pool=[]\n",
    "        pool_start={}\n",
    "        pool_end={}\n",
    "\n",
    "        # populate pool and preprocess all images in one lump\n",
    "        for id in test_video:\n",
    "          pool_start[id] = len(preprocess_pool) \n",
    "          preprocess_pool.extend(test_video[id][::video_step])\n",
    "          pool_end[id] = len(preprocess_pool) \n",
    "\n",
    "        preprocess_result = np.array(preprocess_imgs(preprocess_pool, size_multiplier=self.size_multiplier))\n",
    "        \n",
    "         \n",
    "        for id in test_video:\n",
    "          print('classifying video id ',id )\n",
    "          pca1 = self.pca.transform(get_layer_output(preprocess_result[pool_start[id]:pool_end[id]], layer=self.layer))\n",
    "          # pca2 = self.pca2.transform(get_layer_output(preprocess_result[pool_start[id]:pool_end[id]], layer='fc7'))\n",
    "                                    \n",
    "          x_pred= pca1 # np.hstack((pca1, pca2))\n",
    "          probs = self.knn_model.predict_proba(x_pred)\n",
    "          y_pred= self.label_encoder.inverse_transform(self.knn_model.predict(x_pred))\n",
    " #         neigh_dist, neigh_ind = self.knn_model.kneighbors(x_pred, return_distance=True)\n",
    "          for row in range(probs.shape[0]):\n",
    "         #   print('Frame ',row)\n",
    "            best_indices = probs[row,:].argsort()[-5:][::-1]\n",
    "         #   #print(neigh_dist.shape, neigh_ind.shape)\n",
    "             # print('best indices:', best_indices)\n",
    "            for i in range(5):\n",
    "             print('{} (p={})'.format(self.label_encoder.inverse_transform(best_indices)[i],\n",
    "                                      probs[row,best_indices[i]]))\n",
    "\n",
    "#            # collect sliding window of past nearest neighbours\n",
    "#\n",
    "#            windowstart=max(0,row-10)\n",
    "#            ind_window=np.array(neigh_ind[windowstart:row+1]).flatten() \n",
    "#            dist_window=np.array(neigh_dist[windowstart:row+1]).flatten() \n",
    "#\n",
    "#            ind_window_sorted = ind_window[np.argsort(dist_window)]\n",
    "#            dist_window_sorted = np.sort(dist_window)\n",
    "#\n",
    "#            \n",
    "#            print(neigh_dist[row])\n",
    "#            print(self.label_encoder.inverse_transform(self.y_train_internal[neigh_ind[row]]))\n",
    "#            print(self.label_encoder.inverse_transform(self.y_train_internal[ind_window_sorted]))\n",
    "#            print(dist_window_sorted)\n",
    "\n",
    "            print('Frame prediction: ',y_pred[row])\n",
    "\n",
    "          lst=list(y_pred)\n",
    "          predictions[id]=max(set(lst), key=lst.count) # mode of y_pred\n",
    "          print('prediction for video {}: {}'.format(id,predictions[id]))\n",
    "\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn import decomposition\n",
    "from sklearn import ensemble\n",
    "\n",
    "from skimage.io import imread\n",
    "import cv2\n",
    "from os.path import join\n",
    "class ClassifierRandomForest():\n",
    "    def __init__(self, nn_model):\n",
    "        self.label_encoder = preprocessing.LabelEncoder()\n",
    "        self.rf_model = ensemble.RandomForestClassifier(n_estimators=750,verbose=1)\n",
    "        self.pca1 = decomposition.PCA(0.9)\n",
    "        self.pca2 = decomposition.PCA(0.9)\n",
    "        self.pca3 = decomposition.PCA(0.9)\n",
    "        \n",
    "        self.layer = 'fc6+fc7+fc8'\n",
    "        self.size_multiplier=2.15\n",
    "        self.video_step = 1\n",
    "\n",
    "    def fit(self, train_imgs, train_labels):\n",
    "        n_images=len(train_imgs)\n",
    "        image_counter=0\n",
    "        \n",
    "        labels_orig = []\n",
    "        for id in train_imgs:\n",
    "          labels_orig.append(train_labels[id])\n",
    "        \n",
    "        labels_orig.extend(labels_orig)\n",
    "\n",
    "        preprocessed_images = np.array(preprocess_imgs(train_imgs.values(), size_multiplier=self.size_multiplier))\n",
    "        flipped_images=np.flip(preprocessed_images, -2)\n",
    "        augmented_images = np.vstack((preprocessed_images, flipped_images))\n",
    "\n",
    "        # augment images with two different scaling bottlenecks\n",
    "\n",
    "#        bottleneck_70 = np.zeros_like(augmented_images)\n",
    "#        bottleneck_110 = np.zeros_like(augmented_images)\n",
    "##\n",
    "#        n_augm = augmented_images.shape[0]\n",
    "#        for i in range(n_augm):\n",
    "#        bottleneck_70[i,:,:] = transform.resize(transform.resize(augmented_images[i], (70, 70)), (224, 224))\n",
    "#        bottleneck_110[i,:,:] = transform.resize(transform.resize(augmented_images[i], (110, 110)), (224, 224))\n",
    "\n",
    "#        labels_orig = labels_orig * 3\n",
    "#        augmented_images = np.vstack((augmented_images, bottleneck_70, bottleneck_110))\n",
    "        \n",
    "#        blur1 = np.zeros_like(augmented_images)\n",
    "#        blur2 = np.zeros_like(augmented_images)\n",
    "\n",
    "#        n_augm = augmented_images.shape[0]\n",
    "#        for i in range(n_augm):\n",
    "#         blur1[i,:,:] = filters.gaussian(augmented_images[i])\n",
    "#         blur2[i,:,:] = filters.gaussian(blur1[i])\n",
    "         \n",
    "#        labels_orig = labels_orig * 3\n",
    "#        augmented_images = np.vstack((augmented_images, blur1, blur2))\n",
    "\n",
    "#        print('augmented image set shape {}'.format(augmented_images.shape))\n",
    "\n",
    "#        #flip the images of the second half horizontally\n",
    "\n",
    "        pca1 = self.pca1.fit_transform(get_layer_output(augmented_images, layer='fc6'))\n",
    "        pca2 = self.pca2.fit_transform(get_layer_output(augmented_images, layer='fc7'))\n",
    "        pca3 = self.pca3.fit_transform(get_layer_output(augmented_images, layer='fc8'))\n",
    "        # pca2 = self.pca2.fit_transform(get_layer_output(augmented_images,layer='fc7'))\n",
    "        \n",
    "        self.x_train_internal= np.hstack((pca1,pca2,pca3))\n",
    "        self.y_train_internal = self.label_encoder.fit_transform(labels_orig)\n",
    "        \n",
    "        self.rf_model.fit(self.x_train_internal, self.y_train_internal)\n",
    "        self.fitted=True\n",
    "\n",
    "    def classify_images(self, test_imgs):\n",
    "      assert self.fitted\n",
    "\n",
    "      n_images=len(test_imgs)\n",
    "      image_counter=0\n",
    "              \n",
    "      # x_test= get_layer_output(np.array(preprocess_imgs(test_imgs.values(), size_multiplier=self.size_multiplier)), layer=self.layer)\n",
    "      preprocess_result = np.array(preprocess_imgs(test_imgs.values(), size_multiplier=self.size_multiplier))   \n",
    "      pca1 =  self.pca1.transform(get_layer_output(preprocess_result, layer='fc6'))\n",
    "      pca2 =  self.pca2.transform(get_layer_output(preprocess_result, layer='fc7'))\n",
    "      pca3 =  self.pca3.transform(get_layer_output(preprocess_result, layer='fc8'))\n",
    "      x_test = np.hstack((pca1, pca2, pca3))  \n",
    "      ids=[] \n",
    "      for id in test_imgs:\n",
    "        ids.append(id)\n",
    "        \n",
    "      y_test=self.label_encoder.inverse_transform(self.rf_model.predict(x_test))\n",
    "      print('mapping predictions back')\n",
    "      print(y_test.shape)\n",
    "      for i in range(n_images):\n",
    "        predictions[ids[i]] = str(y_test[i])\n",
    "      return predictions\n",
    "        \n",
    "\n",
    "\n",
    "      \n",
    "\n",
    "        \n",
    "    def classify_videos(self, test_video):\n",
    "        assert self.fitted\n",
    "        predictions={}\n",
    "\n",
    "        video_step = 6\n",
    "\n",
    "        preprocess_pool=[]\n",
    "        pool_start={}\n",
    "        pool_end={}\n",
    "\n",
    "        # populate pool and preprocess all images in one lump\n",
    "        for id in test_video:\n",
    "          pool_start[id] = len(preprocess_pool) \n",
    "          preprocess_pool.extend(test_video[id][::video_step])\n",
    "          pool_end[id] = len(preprocess_pool) \n",
    "\n",
    "        preprocess_result = np.array(preprocess_imgs(preprocess_pool, size_multiplier=self.size_multiplier))\n",
    "        \n",
    "         \n",
    "        for id in test_video:\n",
    "          print('classifying video id ',id )\n",
    "          pca1 = self.pca1.transform(get_layer_output(preprocess_result[pool_start[id]:pool_end[id]], layer='fc6'))\n",
    "          pca2 = self.pca2.transform(get_layer_output(preprocess_result[pool_start[id]:pool_end[id]], layer='fc7'))\n",
    "          pca3 = self.pca3.transform(get_layer_output(preprocess_result[pool_start[id]:pool_end[id]], layer='fc8'))\n",
    "                                    \n",
    "          x_pred= np.hstack((pca1, pca2, pca3))\n",
    "          probs = self.rf_model.predict_proba(x_pred)\n",
    "          y_pred= self.label_encoder.inverse_transform(self.rf_model.predict(x_pred))\n",
    "          for row in range(probs.shape[0]):\n",
    "         #   print('Frame ',row)\n",
    "         #   #best_indices = probs[row,:].argsort()[-5:][::-1]\n",
    "         #   #print(neigh_dist.shape, neigh_ind.shape)\n",
    "         #   #print('best indices:', best_indices)\n",
    "         #   #for i in range(5):\n",
    "         #   #  print('{} (p={})'.format(self.label_encoder.inverse_transform(best_indices)[i],\n",
    "         #   #                           probs[row,best_indices[i]]))\n",
    "\n",
    "#            # collect sliding window of past nearest neighbours\n",
    "#\n",
    "#            windowstart=max(0,row-10)\n",
    "#            ind_window=np.array(neigh_ind[windowstart:row+1]).flatten() \n",
    "#            dist_window=np.array(neigh_dist[windowstart:row+1]).flatten() \n",
    "#\n",
    "#            ind_window_sorted = ind_window[np.argsort(dist_window)]\n",
    "#            dist_window_sorted = np.sort(dist_window)\n",
    "#\n",
    "#            \n",
    "#            print(neigh_dist[row])\n",
    "#            print(self.label_encoder.inverse_transform(self.y_train_internal[neigh_ind[row]]))\n",
    "#            print(self.label_encoder.inverse_transform(self.y_train_internal[ind_window_sorted]))\n",
    "#            print(dist_window_sorted)\n",
    "\n",
    "            print('Frame prediction: ',y_pred[row])\n",
    "\n",
    "          lst=list(y_pred)\n",
    "          predictions[id]=max(set(lst), key=lst.count) # mode of y_pred\n",
    "          print('prediction for video {}: {}'.format(id,predictions[id]))\n",
    "\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IdT5_xfVsqSu"
   },
   "outputs": [],
   "source": [
    "len(video_test['0'][::2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DAgG4UyJoL6Q"
   },
   "outputs": [],
   "source": [
    "# %%capture cap_out --no-stderr\n",
    "\n",
    "def extract_subdict(d, keys):\n",
    "    return dict((k, d[k]) for k in keys if k in d)\n",
    "\n",
    "difficult_labels=['10','14','16','29','35','40','43','49','5','50','53','54','57','59','63']\n",
    "testing_labels = ['0','1','11']+difficult_labels\n",
    "video_classifier = ClassifierVideo(model)\n",
    "video_classifier.fit(video_train, train_labels)\n",
    "y_video_difficult_out = video_classifier.classify_videos(extract_subdict(video_test, testing_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zsTLjxSpthaS"
   },
   "outputs": [],
   "source": [
    "with open('fc7-heq-2.15.txt', 'w') as f:\n",
    "    f.write(cap_out.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture cap_out --no-stderr\n",
    "\n",
    "#video_train_aug=video_train\n",
    "#train_labels_aug=train_labels\n",
    "\n",
    "#for lbl in x_train:\n",
    "#    video_train_aug['img:'+lbl] = x_train[lbl]\n",
    "#    train_labels_aug['img:'+lbl] = y_train[lbl]\n",
    "\n",
    "video_classifier = ClassifierVideoCatboost(model)\n",
    "video_classifier.fit(video_train, train_labels)\n",
    "#video_classifier.fit(video_train_aug, train_labels_aug)\n",
    "y_video_out = video_classifier.classify_videos(video_test)\n",
    "\n",
    "print(check_test(y_video_out, test_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#video_train_aug=video_train\n",
    "#train_labels_aug=train_labels\n",
    "\n",
    "#for lbl in x_train:\n",
    "#    video_train_aug['img:'+lbl] = x_train[lbl]\n",
    "#    train_labels_aug['img:'+lbl] = y_train[lbl]\n",
    "\n",
    "video_classifier = ClassifierRandomForest(model)\n",
    "video_classifier.fit(video_train, train_labels)\n",
    "#video_classifier.fit(video_train_aug, train_labels_aug)\n",
    "y_video_out = video_classifier.classify_videos(video_test)\n",
    "\n",
    "print(check_test(y_video_out, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xHRr5eHf8cS7"
   },
   "outputs": [],
   "source": [
    "# %%capture cap_out --no-stderr\n",
    "\n",
    "video_train_aug=video_train\n",
    "train_labels_aug=train_labels\n",
    "\n",
    "for lbl in x_train:\n",
    "    video_train_aug['img:'+lbl] = x_train[lbl]\n",
    "    train_labels_aug['img:'+lbl] = y_train[lbl]\n",
    "\n",
    "video_classifier = ClassifierVideo(model)\n",
    "#video_classifier.fit(video_train, train_labels)\n",
    "video_classifier.fit(video_train_aug, train_labels_aug)\n",
    "y_video_out = video_classifier.classify_videos(video_test)\n",
    "\n",
    "print(check_test(y_video_out, test_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "khAmArs3p5Tt"
   },
   "outputs": [],
   "source": [
    "y_video_out = video_classifier.classify_videos(video_test)\n",
    "\n",
    "print(check_test(y_video_out, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uAoP8fz0VNi-"
   },
   "outputs": [],
   "source": [
    "with open('fc7-heq-bottleneck-2.15-all.txt', 'w') as f:\n",
    "    f.write(cap_out.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "wSLl0mpb8cS-",
    "outputId": "d1b4c514-124a-4aca-d7db-c897ab7b0ed5"
   },
   "outputs": [],
   "source": [
    "print(check_test(y_video_out, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IGj2CiU78cTB"
   },
   "outputs": [],
   "source": [
    "video_classifier.pca.n_components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "g7qWFs1BgEwz",
    "outputId": "d2eec45c-2e40-4a7d-f352-b60113567bc0"
   },
   "outputs": [],
   "source": [
    "img_classifier = ClassifierVideo(model)\n",
    "img_classifier.fit(x_train, y_train)\n",
    "y_out = img_classifier.classify_images(x_test)\n",
    "print(check_test(y_out, y_test))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "ace Recognition task - video tests.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
